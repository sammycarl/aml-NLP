{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mainNLP_local_version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sammycarl/aml-NLP/blob/master/mainNLP_local_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMaatzUtxe7I",
        "colab_type": "text"
      },
      "source": [
        "# Pre stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rO0gXIJLCI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Oxford/Advanced Machine Learning/Python_files')\n",
        "import Encoder_v2\n",
        "import CoattentionEncoder_v2\n",
        "import Dynamic_Pointing_Decoder\n",
        "import Config_file "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xi6GRn9LJu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.cuda\n",
        "from copy import copy\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv as csv\n",
        "import torch.optim as optimizer\n",
        "import math \n",
        "import random \n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "import time\n",
        "import sys\n",
        "import pickle\n",
        "from statistics import mean\n",
        "sys.path.append('/content/drive/My Drive/Oxford/Advanced Machine Learning/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dkXSBOMLLRy",
        "colab_type": "code",
        "outputId": "45fe94b5-1654-4546-a1e2-3c478f993b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#check if cuda is available\n",
        "#if it is available, set the default tensor to cuda \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device == torch.device(\"cuda:0\"):\n",
        "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "else: \n",
        "  torch.set_default_tensor_type(torch.FloatTensor)\n",
        "#torch.randn((1,2)).long().is_cuda\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzFMipm5LPRU",
        "colab_type": "code",
        "outputId": "483f5594-54a6-4ff7-9631-c1712ad34e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/glove_words_used_pd.txt', 'r') as glove:\n",
        "  glove_data_file = glove\n",
        "  words = pd.read_table(glove_data_file, sep=\",\", index_col=0, header=None, quoting=csv.QUOTE_ALL)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI think that when we define the class of the Encoder the embedding matrix is the whole glove file\\nWhen we use 'Embeddings' in pytorch each word has a key that can be used for lookup. \\nhttps://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\\nhttps://towardsdatascience.com/pre-trained-word-embeddings-or-embedding-layer-a-dilemma-8406959fd76c \\nhttps://github.com/joosthub/pytorch-nlp-tutorial-ny2018/blob/master/docs/recipes/load_pretrained_vectors.rst \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXcNdBSaLhJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create the embedding matrix: np matrix of shape vocab_size x embedding dim\n",
        "embedding_matrix = words.to_numpy()\n",
        "#add a zero vector that represents words that are not in glove \n",
        "zero_vector = np.zeros(300)\n",
        "embedding_matrix = np.vstack([embedding_matrix, zero_vector])\n",
        "\n",
        "#create a word_to_index dictionary: key is word and value is index in embedding matrix\n",
        "#create a index_to_word dictionary: key is index in embeddingmatrix  and value is word\n",
        "word_to_index = dict()\n",
        "index = 0 \n",
        "for word in words.index.values:\n",
        "  word_to_index[word]=index\n",
        "  index = index+1\n",
        "\n",
        "index_to_word = dict((v,k) for k,v in word_to_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRSO38sgLnw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_batches(data):\n",
        "  return [data[i * Config_file.batch_size:(i + 1) * Config_file.batch_size] for i in range((len(data) + Config_file.batch_size - 1) // Config_file.batch_size )]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koz8kcGBLowe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, embedding_matrix, hidden_dim, dropout_ratio, num_hidden_layers, input_size, hidden_size, u_embedding_dim,device):\n",
        "    super(Model, self).__init__()\n",
        "    self.Encoder = Encoder_v2.Encoder(embedding_matrix, hidden_dim, dropout_ratio, num_hidden_layers, device)\n",
        "    self.CoattentionEncoder = CoattentionEncoder_v2.CoattentionEncoder(input_size, hidden_size, dropout_ratio, num_hidden_layers,device)\n",
        "    self.Decoder = Dynamic_Pointing_Decoder.Dynamic_Pointing_Decoder(u_embedding_dim, hidden_dim, num_hidden_layers, dropout_ratio,device)\n",
        "\n",
        "  def forward(self, batch_input_context, batch_context_lengths, batch_input_question, batch_question_lengths, training, padding): \n",
        "    start_DEncoding = time.time()\n",
        "    context_encoding = self.Encoder.forward(batch_input_context, training, padding, question=False)\n",
        "    end_DEncoding = time.time()\n",
        "    #print(\"Time D encoding\", end_DEncoding-start_DEncoding)\n",
        "\n",
        "    start_QEncoding = time.time()\n",
        "    question_encoding = self.Encoder.forward(batch_input_question, training, padding, question=True)\n",
        "    end_QEncoding = time.time()\n",
        "    #print(\"Time Q encoding\", end_QEncoding-start_QEncoding)\n",
        "\n",
        "    \n",
        "    start_CoaEnc = time.time()\n",
        "    U = self.CoattentionEncoder.forward( context_encoding,question_encoding, training, padding )\n",
        "    end_CoaEnc = time.time()\n",
        "    #print(\"Time  CoaEncoder\", end_CoaEnc-start_CoaEnc)\n",
        "\n",
        "    \n",
        "    start_answer = time.time()\n",
        "    #@sam add padding argument \n",
        "    start_indices, end_indices, start_scores_tensor, end_scores_tensor = self.Decoder.forward(U, training, padding, batch_context_lengths)\n",
        "    end_answer = time.time()\n",
        "    #print(\"Time dynamic coattention decoder\", end_answer-start_answer)\n",
        "    return start_indices, end_indices, start_scores_tensor, end_scores_tensor\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HX3jkWsRxcwR"
      },
      "source": [
        "#Config/ Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3AbpzzpLsyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Config_file.epochs = 200\n",
        "Config_file.batch_size = 32\n",
        "Config_file.dropout_ratio = 0\n",
        "Config_file.learning_rate = 0.002\n",
        "Config_file.num_hidden_layers = 1\n",
        "Config_file.hidden_dim = 200\n",
        "\n",
        "saveEveryXEpochs = 50 \n",
        "epoch_left_off = 0\n",
        "epoch_losses = []\n",
        "\n",
        "number_to_train = 24000 \n",
        "start_from = 0\n",
        "\n",
        "experimenter = \"Fleur\" #Change this to your name when you run an experiment\n",
        "experiment = \"dr0.0\" #\n",
        "hyperparameters = {'batch_size':Config_file.batch_size,\n",
        "                   'learning_rate':Config_file.learning_rate,\n",
        "                   'dropout_ratio': Config_file.dropout_ratio,\n",
        "                   'Number_to_train': number_to_train,\n",
        "                   'Number_hidden_layers': Config_file.num_hidden_layers,\n",
        "                   'hidden_dim': Config_file.hidden_dim,\n",
        "                   'MAXITERATIONS':Config_file.MAXITERATIONS,\n",
        "                   'MAXOUT_LAYER_POOLSIZE':Config_file.MAXOUT_LAYER_POOLSIZE,\n",
        "                   'epochs' : Config_file.epochs}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nVFcAQfLz7e",
        "colab_type": "text"
      },
      "source": [
        "# Loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEl08d60Lxu6",
        "colab_type": "code",
        "outputId": "4ce2d00b-f588-497a-8934-d638b253933e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "u_embedding_dim = 2*embedding_matrix.shape[1]\n",
        "#Also \"Hidden_size\" for now I have used hidden_dim\n",
        "input_size = 3* embedding_matrix.shape[1]\n",
        "\n",
        "model = Model(embedding_matrix, Config_file.hidden_dim, Config_file.dropout_ratio, Config_file.num_hidden_layers, input_size, Config_file.hidden_dim, u_embedding_dim,device).to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "filtered_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "adam_optimizer = optimizer.Adam(filtered_params, Config_file.learning_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY6m0OXKL3JG",
        "colab_type": "text"
      },
      "source": [
        "##Loading from a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj2sHyBXL1fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = torch.load(\"/content/drive/My Drive/Oxford/Advanced Machine Learning/Models/SamTestingsizedata64\")\n",
        "\n",
        "hyperparameters = checkpoint['hyperparameters']\n",
        "\n",
        "Config_file.epochs = hyperparameters['epochs']\n",
        "Config_file.batch_size = hyperparameters['batch_size']\n",
        "Config_file.dropout_ratio = hyperparameters['dropout_ratio']\n",
        "Config_file.learning_rate = hyperparameters['learning_rate'] #Fleur/ Adri keep this, Sam 0.01\n",
        "Config_file.MAXITERATIONS = hyperparameters['MAXITERATIONS']\n",
        "Config_file.MAXOUT_LAYER_POOLSIZE = hyperparameters['MAXOUT_LAYER_POOLSIZE']\n",
        "Config_file.num_hidden_layers = hyperparameters['Number_hidden_layers']\n",
        "Config_file.hidden_dim = hyperparameters['hidden_dim']\n",
        "\n",
        "number_to_train = hyperparameters[\"Number_to_train\"]\n",
        "\n",
        "model = Model(embedding_matrix, Config_file.hidden_dim, Config_file.dropout_ratio, Config_file.num_hidden_layers, input_size, Config_file.hidden_dim, u_embedding_dim,device).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "filtered_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "\n",
        "adam_optimizer = optimizer.Adam(filtered_params, Config_file.learning_rate)\n",
        "adam_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "model.train()\n",
        "\n",
        "epoch_left_off = checkpoint['epoch']\n",
        "epoch_losses =  checkpoint['epoch_losses']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6cTXZQ3MFNJ",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itdNTBlYL698",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(\"Starting, config = ...\")\n",
        "padding = False\n",
        "training = True \n",
        "\n",
        "model.train() #Make sure it's in training mode\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/train.span', 'r') as span:\n",
        "  allspans = span.readlines()\n",
        "\n",
        "spans = allspans[start_from:start_from+number_to_train]\n",
        "print(hyperparameters)\n",
        "\n",
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/Saved_Files_Test/traincontexts_lengths_1to5', 'rb') as traincontexts_lengths_file:\n",
        "    contexts_lengths = pickle.load(traincontexts_lengths_file)[start_from:start_from+number_to_train]\n",
        "\n",
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/Saved_Files_Test/trainquestions_lengths_1to5', 'rb') as trainquestions_lengths_file:\n",
        "    questions_lengths = pickle.load(trainquestions_lengths_file)[start_from:start_from+number_to_train]\n",
        "                                                                      \n",
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/Saved_Files_Test/traincontexts_indices_1to5', 'rb') as traincontexts_indices_file:\n",
        "    contexts_indices = pickle.load(traincontexts_indices_file)[start_from:start_from+number_to_train]\n",
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/Saved_Files_Test/trainquestions_indices_1to5', 'rb') as trainquestions_indices_file:\n",
        "    questions_indices = pickle.load(trainquestions_indices_file)[start_from:start_from+number_to_train]\n",
        "\n",
        "batch_losses = []\n",
        "epoch_loss = 0 if len(epoch_losses)==0 else epoch_losses[-1]\n",
        "\n",
        "for epoch in range(epoch_left_off, Config_file.epochs): \n",
        "  print(\"starting epoch: \" + str(epoch+1) + \" of: \" + str(Config_file.epochs))\n",
        "  \n",
        "  if epoch%saveEveryXEpochs==0:\n",
        "    FILE_PATH = \"/content/drive/My Drive/Oxford/Advanced Machine Learning/Models/\" + experimenter+experiment + \"epochs\" +str(epoch)+\"sizedata\"+str(number_to_train)\n",
        "    torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': adam_optimizer.state_dict(),\n",
        "              'loss': epoch_loss,\n",
        "              'epoch_losses': epoch_losses,\n",
        "              'hyperparameters': hyperparameters\n",
        "              }, FILE_PATH)\n",
        "    \n",
        "  if epoch%5==0:\n",
        "    FILE_PATH = \"/content/drive/My Drive/Oxford/Advanced Machine Learning/Models/\" + experimenter+experiment+\"sizedata\"+str(number_to_train)\n",
        "    torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': adam_optimizer.state_dict(),\n",
        "              'loss': epoch_loss,\n",
        "              'epoch_losses': epoch_losses,\n",
        "              'hyperparameters': hyperparameters\n",
        "            }, FILE_PATH)\n",
        "\n",
        "  timeEpochStuff = time.time()\n",
        "  temp = list(zip( contexts_indices, questions_indices,contexts_lengths,questions_lengths,spans))\n",
        "  random.shuffle(temp)\n",
        "  contexts_indices, questions_indices,contexts_lengths,questions_lengths,spans = zip(*temp)\n",
        "\n",
        "  #divide the training data in batches of batch_size\n",
        "  batches_contexts_indices = to_batches(contexts_indices)\n",
        "  batches_contexts_lengths = to_batches(contexts_lengths)\n",
        "  batches_questions_indices = to_batches(questions_indices)\n",
        "  batches_questions_lengths = to_batches(questions_lengths)\n",
        "  batches_spans = to_batches(spans)\n",
        "  #print(\"epoch stuff\", time.time()- timeEpochStuff)\n",
        "  #loop through the batches\n",
        "  for i in tqdm(range(len(batches_contexts_indices ))):\n",
        "    if padding:\n",
        "      batch_context_indices = list(add_padding(batches_contexts_indices[i]))\n",
        "      batch_question_indices = list(add_padding(batches_questions_indices[i]))\n",
        "    else: \n",
        "      batch_context_indices = [torch.Tensor(batches_contexts_indices[i][k]) for k in range (len(batches_contexts_indices[i])) ]\n",
        "      batch_question_indices = [torch.Tensor(batches_questions_indices[i][k]) for k in range (len(batches_questions_indices[i])) ]\n",
        "    #backward() function accumulates gradients and we dont want to mix up gradients between minibatches. \n",
        "    #thus we want to set the to zero at the start of a new minibatch\n",
        "    adam_optimizer.zero_grad()\n",
        "  \n",
        "\n",
        "    timeForwardPass= time.time()\n",
        "    batch_start_indices, batch_end_indices, batch_start_scores_tensor, batch_end_scores_tensor = model(batch_context_indices, list(batches_contexts_lengths[i]), batch_question_indices, list(batches_questions_lengths[i]), training, padding)\n",
        "    \n",
        "    #print(\"timeFor1ForwardPass = \", time.time()- timeForwardPass)\n",
        "    timeLossCalc = time.time()\n",
        "\n",
        "    true_start_and_ends = batches_spans[i]\n",
        "    iter_through_batch = 0 #Something to iterate through loop\n",
        "    loss_start_indices = 0\n",
        "    loss_end_indices = 0\n",
        "    batch_score = 0\n",
        "    for true_start_unopened in true_start_and_ends:\n",
        "      true_start, true_end = true_start_unopened.split()\n",
        "      true_start = int(true_start)\n",
        "      true_end = int(true_end)\n",
        "      true_start_tensor = torch.Tensor([true_start]*batch_start_scores_tensor[iter_through_batch].shape[0]).long()\n",
        "      true_end_tensor = torch.Tensor([true_end]*batch_end_scores_tensor[iter_through_batch].shape[0]).long()\n",
        "      #print(\"true_end  \", true_end)\n",
        "      #print(\"true_end_tensor \",true_end_tensor)\n",
        "      #print(\"true_end_tensor \",true_end_tensor.shape)\n",
        "      #print(\"batch_end_scores_tensor[iter_through_batch] \",batch_end_scores_tensor[iter_through_batch])\n",
        "      #print(\"batch_end_scores_tensor[iter_through_batch] \",batch_end_scores_tensor[iter_through_batch].shape)\n",
        "\n",
        "      if true_start == batch_start_indices[iter_through_batch] and true_end == batch_end_indices[iter_through_batch]:\n",
        "        batch_score += 1\n",
        "      loss_start_indices += loss_function(batch_start_scores_tensor[iter_through_batch],true_start_tensor)\n",
        "      loss_end_indices += loss_function(batch_end_scores_tensor[iter_through_batch],true_end_tensor)\n",
        "\n",
        "      iter_through_batch+=1\n",
        "\n",
        "    \n",
        "    #add the losses of the start indices and end indices \n",
        "    loss = loss_start_indices + loss_end_indices\n",
        "    batch_losses.append(loss.item())\n",
        "    \n",
        "    #loss.requires_grad = True\n",
        "    #calculate the gradients using the backward() method of the lossfunction \n",
        "    timeLossBack = time.time()\n",
        "    loss.backward()\n",
        "    #print(\"loss.backward \", time.time()- timeLossBack)\n",
        "    #update the parameters using the step() method of the optimizer \n",
        "    adam_optimizer.step()\n",
        "    #print(\"loss calculations\", time.time()- timeLossCalc)\n",
        "  \n",
        "  epoch_loss = mean(batch_losses)\n",
        "  epoch_losses.append(epoch_loss)\n",
        "  print(\"Mean epoch loss \", epoch_loss)\n",
        "  batch_losses = []\n",
        "\n",
        "FILE_PATH = \"/content/drive/My Drive/Oxford/Advanced Machine Learning/Models/\" + experimenter + experiment + \"epochs\" +str(epoch)+\"sizedata\"+str(number_to_train) + \"End\"\n",
        "torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': adam_optimizer.state_dict(),\n",
        "          'loss': epoch_loss,\n",
        "          'epoch_losses': epoch_losses,\n",
        "          'hyperparameters': hyperparameters\n",
        "          }, FILE_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJlC32jwMI9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_losses\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for plotting\n",
        "epochs = range(len(epoch_losses))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(epochs, epoch_losses)\n",
        "\n",
        "ax.set(xlabel='epoch', ylabel='loss',\n",
        "       title='')\n",
        "ax.grid()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpd19bmxMHgH",
        "colab_type": "text"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAeugUaiMM6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = False \n",
        "\n",
        "startValidationSet= 0\n",
        "endValidationSet = 16\n",
        "\n",
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/Saved_Files_Test/traincontexts_lengths_1to5', 'rb') as traincontexts_lengths_file:\n",
        "    contexts_lengths = pickle.load(traincontexts_lengths_file)[startValidationSet:endValidationSet]\n",
        "\n",
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/Saved_Files_Test/trainquestions_lengths_1to5', 'rb') as trainquestions_lengths_file:\n",
        "    questions_lengths = pickle.load(trainquestions_lengths_file)[startValidationSet:endValidationSet]\n",
        "                                                                      \n",
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/Saved_Files_Test/traincontexts_indices_1to5', 'rb') as traincontexts_indices_file:\n",
        "    # read the data as binary data stream\n",
        "    contexts_indices = pickle.load(traincontexts_indices_file)[startValidationSet:endValidationSet]\n",
        "with open('/content/drive/My Drive/Oxford/Advanced Machine Learning/data/Saved_Files_Test/trainquestions_indices_1to5', 'rb') as trainquestions_indices_file:\n",
        "    # read the data as binary data stream\n",
        "    questions_indices = pickle.load(trainquestions_indices_file)[startValidationSet:endValidationSet]\n",
        "\n",
        "spans = allspans[startValidationSet:endValidationSet]\n",
        "epoch_losses = []\n",
        "batch_losses = []\n",
        "\n",
        "#divide the training data in batches of batch_size\n",
        "batches_contexts_indices = to_batches(contexts_indices)\n",
        "batches_contexts_lengths = to_batches(contexts_lengths)\n",
        "batches_questions_indices = to_batches(questions_indices)\n",
        "batches_questions_lengths = to_batches(questions_lengths)\n",
        "batches_spans = to_batches(spans)\n",
        "\n",
        "exact_match_score = 0\n",
        "f1_score = 0\n",
        "for i in tqdm(range(len(batches_contexts_indices ))):\n",
        "  if padding:\n",
        "    batch_context_indices = list(add_padding(batches_contexts_indices[i]))\n",
        "    batch_question_indices = list(add_padding(batches_questions_indices[i]))\n",
        "  else: \n",
        "    batch_context_indices = [torch.Tensor(batches_contexts_indices[i][k]) for k in range (len(batches_contexts_indices[i])) ]\n",
        "    batch_question_indices = [torch.Tensor(batches_questions_indices[i][k]) for k in range (len(batches_questions_indices[i])) ]\n",
        "\n",
        "  model.eval()\n",
        "  batch_start_indices, batch_end_indices, _ , _ = model(batch_context_indices, list(batches_contexts_lengths[i]), batch_question_indices, list(batches_questions_lengths[i]), training, padding)\n",
        "\n",
        "  true_start_and_ends = batches_spans[i]\n",
        "  iter_through_batch = 0 #Something to iterate through loop\n",
        "  batch_exact_match_score = 0\n",
        "  for true_start_unopened in true_start_and_ends:\n",
        "    true_start, true_end = true_start_unopened.split()\n",
        "    true_start = int(true_start)\n",
        "    true_end = int(true_end)\n",
        "\n",
        "    if true_start == batch_start_indices[iter_through_batch] and true_end == batch_end_indices[iter_through_batch]:\n",
        "      batch_exact_match_score += 1\n",
        "    \n",
        "    iter_through_batch+=1\n",
        "  exact_match_score+=batch_exact_match_score\n",
        "\n",
        "\n",
        "print(exact_match_score)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}